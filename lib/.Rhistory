fit_updated = update(fit, . ~ . - z3 - z4 -z5)
Manova(fit_updated)
mc_updated = update(mc, . ~ . - pr - diap -qrs)
Manova(mc_updated)
```{r}
par(mfrow=c(1,2))
qqnorm(residuals(mc_updated))
plot(fitted(mc_updated),residuals(mc_updated))
abline(h=0)
install.packages("leaps")
n<-nrow(amit)
m<-2
r<-5
Z = cbind(rep(1,17),as.matrix(amit[1:17,3:7]))
Y = as.matrix(amit[1:17,1:2])
invZTZ = solve(crossprod(Z))
whole = lm(cbind(bl,em,sf,bs)~afl+lff+fff+zst,data=pulp)
Manova(whole)
fit = lm(cbind(y1, y2) ~ ., data = ami)
Manova(fit)
fit_updated = update(fit, . ~ . - z3 - z4 -z5)
Manova(fit_updated)
mb<-lm(ami~gen+amt+pr+diap+qrs,data=amit)
step(mb,scope=c(lower=~1)),direction="backward")
mb<-lm(ami~gen+amt+pr+diap+qrs,data=amit)
step(mb,scope=c(lower=~1))direction="backward")
mb<-lm(ami~gen+amt+pr+diap+qrs,data=amit)
step(mb,scope=c(lower=~1),direction="backward")
step(mb,scope=c(lower=~1),direction="backward",steps = 2)
mb<-lm(ami~gen+amt+pr+diap+qrs,data=amit)
step(mb,scope=c(lower=~1),direction="backward",steps = 4)
mb<-lm(ami~gen+amt+pr+diap+qrs,data=amit)
m.bacck<-step(mb,scope=c(lower=~1),direction="backward",steps = 4)
mb<-lm(ami~gen+amt+pr+diap+qrs,data=amit)
m.bacck<-step(mb,scope=c(lower=~1),direction="backward",steps = 4)
m2 = lm(em~afl+lff+fff+zst,data=pulp)
step(m2,direction="backward")
pulp<-read.table("http://users.stat.umn.edu/~guxxx192/courses/wichern_data/t7-7.dat", sep = "", header = FALSE,col.names = c('bl','em','sf','bs','afl','lff','fff','zst'))
m1 = lm(bl~afl+lff+fff+zst,data=pulp)
m1.back<-step(m1,direction="backward")
m2 = lm(em~afl+lff+fff+zst,data=pulp)
m2.back<-step(m2,direction="backward")
m3 = lm(sf~afl+lff+fff+zst,data=pulp)
m3.back<-step(m3,direction="backward")
m4 = lm(bs~afl+lff+fff+zst,data=pulp)
m4.back<-step(m4,direction="backward")
m4.back
m2.back
m1 = lm(bl~afl+lff+fff+zst,data=pulp)
m1.back<-step(m1,direction="backward")
m2 = lm(em~afl+lff+fff+zst,data=pulp)
m2.back<-step(m2,direction="backward")
m3 = lm(sf~afl+lff+fff+zst,data=pulp)
m3.back<-step(m3,direction="backward")
m4 = lm(bs~afl+lff+fff+zst,data=pulp)
m4.back<-step(m4,direction="backward")
par(mfrow=c(2,2))
plot(m1.back)
plot(m2.back)
plot(m3.back)
forbes<-read.table("http://users.stat.umn.edu/~guxxx192/courses/wichern_data/p1-4.dat", sep = "", header = FALSE,col.names = c("sales","profits","assets"))
lm_fit <- lm(profits ~ sales + assets, data = forbes)
library(leaps)
forbes<-read.table("http://users.stat.umn.edu/~guxxx192/courses/wichern_data/p1-4.dat", sep = "", header = FALSE,col.names = c("sales","profits","assets"))
lm_fit <- lm(profits ~ sales + assets, data = forbes)
library(leaps)
Cpplot <- function(cp, ...) {
p <- max(cp$size)
plot(cp$size, cp$Cp, xlab = "p", ylab = "Cp", type = "n")
labels <- apply(
cp$which, 1,
function(x) {
if (any(x) == FALSE) return("(0)")
paste0("(", paste(as.character((1:(p - 1))[x]), collapse = ", "), ")")
}
)
points(cp$size, cp$Cp, pch=16)
text(cp$size, cp$Cp, labels, ...)
abline(0, 1)
}
exhaust <- leaps(forbes[c(1,3)], forbes[, 2])
# calculate Cp for the intercept model
fit0 = lm(profits ~ 1, data = forbes)
Cp0 = sum(fit0$residuals^2)/(summary(lm_fit)$sigma^2) -
(nrow(forbes) - 2 * 1)
exhaust$which = rbind(rep(FALSE,2), exhaust$which)
rownames(exhaust$which)[1] = "0"
exhaust$size = c(1, exhaust$size)
exhaust$Cp = c(Cp0, exhaust$Cp)
Cpplot(exhaust, pos = c(1, 4, 1, 3), cex = 0.7)
title: "Statistical Methods to Handle Missing Data"
author: "Tianfan Song"
date: "March 20, 2016"
output: word_document
---
Statistical Methods to Handle Missing Data
Abstract:
In this paper three types of missing data will be addressed and given definition and example. And this paper will review traditional and modern approach for dealing with these situations. When different methods will be used and what their advantage and disadvantage are will be discussed.
Key word: MCAR, MAR, MNAR, Traditional method, Modern method, Comparison
Introduction:
Missing data is a not a rare case in most datasets. There are typically three different type of missing data: missing at random, missing completely at random, and missing not at random. However, many analytic procedures, many of which were developed early in the twentieth century, were designed to have complete data (Graham, 2009, p550). Then, in late twentieth century different approach to handle missing data were brought up with. Among all these methods, likewise deletion, pairwise deletion, and single imputation as traditional approaches and multiple imputation and maximum likelihood as modern approaches are used a lot.
Type of Missing data:
Missing at random(MAR): If the probability of failing to observe a value does not depend on the unobserved data, then we call this type of missing data as MAR. In other words, according to Rubin,$P(R|Y_{com})=P(R|Y_{obs})$, where $Y_{com}$ means the complete data, whcich is equal to $(Y_{obs},Y_{mis})$ and $Y_{obs}$ $Y_{obs}$ means the observed part and missing part respectively.(as cited in Schafer & Graham, 2002, p.151)
original:
```{r}
independent <- read.csv("data.csv", header=TRUE)
independent.sub1 <- independent[, c(1,2,4)]
independent.sub2 <- independent[, c(1,4,5)]
independent.sub3 <- independent[, c(1,4,6)]
independent.sub4 <- independent[, c(1,4,7)]
original<-lm(sbp~age*gender,data=independent.sub1)
summary(original)
```
Mar
```{r}
marls<-lm(MAR.systolic.bp~age*gender,data=independent.sub2)
summary(marls)
contrasts(factor(independent.sub2,levels=c("m","f")))
```
```{r}
library(mice)
md.pattern(independent.sub2)
imp<-mice(data = contract[,c("Operating_Revenue","Outpatient_Gross_Revenue","Volume2012","Acute_beds","Acuity_index","Market_prominence","Occupancy_rate","pricing_index11","pricing_index13")], seed = 22)
print(imp)
head(imp$imp$MAR.systolic.bp,n=7)
dat.imp<-complete(imp)
head(dat.imp, n=10)
fit.sbp<-with(data=imp,exp=lm(MAR.systolic.bp~age*gender))
fit.sbp
est.p<-pool(fit.sbp)
print(est.p)
summary(est.p)
imp<-mice(independent.sub2,m=5,maxit=50,meth='pmm',seed=500)
print(imp)
head(imp$imp$MAR.systolic.bp,n=7)
dat.imp<-complete(imp)
head(dat.imp, n=10)
fit.sbp<-with(data=imp,exp=lm(MAR.systolic.bp~age*gender))
fit.sbp
est.p<-pool(fit.sbp)
print(est.p)
summary(est.p)
```
Mcar
```{r}
mcarls<-lm(MCAR.systolic.bp~age*gender,data=independent.sub3)
summary(mcarls)
```
for mi
```{r}
imp<-mice(independent.sub3,m=5,maxit=50,meth='pmm',seed=500)
print(imp)
head(imp$imp$MCAR.systolic.bp,n=7)
dat.imp<-complete(imp)
head(dat.imp, n=10)
fit.sbp<-with(data=imp,exp=lm(MCAR.systolic.bp~age*gender))
fit.sbp
est.p<-pool(fit.sbp)
print(est.p)
summary(est.p)
```
MNCR
```{r}
mncarls<-lm(MNAR.systolic.bp~age*gender,data=independent.sub4)
summary(mncarls)
```
Mi
```{r}
tempData<-mice(independent.sub4,m=5,maxit=50,meth='pmm',seed=500)
summary(tempData)
print(tempData$imp$new)
modelFit1 <- with(tempData,lm(MNAR.systolic.bp~age*gender))
summary(pool(modelFit1))
```
$\hat{Z}=b_{0}+b_{1}X+b_{2}{Y}+sE$
$L(\theta)=\prod\limits_{i=1}^{n}f_{i}(y_{i1}, y_{i2},..y_{ik};\theta)$ for observation i
$f_{i}(y_{i2}, y_{i3},..y_{ik};\theta)=\sum_{y_{1}}f_{i}(y_{i1}, y_{i2},..y_{ik};\theta)$
$f_{i}(y_{i2}, y_{i3},..y_{ik};\theta)=\int_{y_{1}}f_{i}(y_{i1}, y_{i2},..y_{ik};\theta)dy_{1}$
$L(\theta)=\prod\limits_{i=1}^{m-n}f_{i}(y_{i1}, y_{i2},..y_{ik};\theta)\prod\limits_{i=m-n+1}^{n}f_{i}( y_{i2},..y_{ik};\theta)$
independent.sub2$gender<-as.numeric(
as.character(
factor(
independent.sub2$gender,
levels = c("m", "f"),
labels = c("1", "2"))))
z <- matrix(rep(1:9), nrow = 3)
colnames(z) <- c("First", "Second", "Third")
z
z[2:3, "Third"]
c(z[,-(2:3)], "abc")
rbind(z[1,], 1:3)
## Section V: More on Vectors and Matrices ##
# Example 1
x <- c(49.3,59.3,68.3,48.1,57.61,78.1,76.1)
y <- c(1894,2050,2353,1838,1948,2528,2568)
n=length(x) # Sample size
n
max(x)
sd(x)
summary(x) # Summary statistics
summary(y)
# Vectorized Operations
u <- c(1,3,5)
v <- c(1,3,5)
v + 4 # Recycling
v + c(1,3) # Recycling
v + u
u <- c(1,3,5)
v <- c(1,3,5)
'+'(v,u)
'*'(v,u)
u <- c(1,3,5)
v <- c(1,3,5)
v + 4 # Recycling
v + c(1,3) # Recycling
u <- c(1,3,5)
v <- c(1,3,5)
v + 4 # Recycling
v + c(1,3) # Recycling
v + u
u <- c(1,3,5)
v <- c(1,3,5)
'+'(v,u)
'*'(v,u)
# Example 1 again
plot(x,y, xlab = "Body Mass", ylab = "Energy Expenditure")
dev_x <- x - mean(x) # First, compute x and y deviations
dev_y <- y - mean(y)
Sxy <- sum(dev_x * dev_y) # Next, compute sum of squares of xy and xx
Sxx <- sum(dev_x * dev_x)
Sxy/Sxx # Compute the estimated slope
mean(y) - (Sxy/Sxx) * mean(x) # Compute the estimated intercept
# Example 2
A <- matrix(c(3,1,1,-2,1/2,1,1,-12,3), nrow = 3)
b <- c(-1, 2, 3)
solve(A, b)
x <- c(1, 2, 0) # Define solution vector x
A %*% x         # Then check with matrix multiplication
# Element-wise matrix operations
A <- matrix(c(1, -2, -2, 4), nrow = 2, byrow = TRUE)
identity <- diag(2) # Define a 2 by 2 identity matrix
identity
det(A - 5*identity) # Check if 5 is an eigenvalue of A
## Section VI: NULL and NA Values
length(c(-1, 0, NA, 5))
length(c(-1, 0, NULL, 5))
t <- c(-1,0,NA,5)
mean(t)
mean(t, na.rm = TRUE) # Use na.rm = TRUE to remove NA values
s <- c(-1, 0, NULL, 5) # NA values are missing, but NULL values don't exist.
mean(s)
# NULL can be used to pre-allocation space
x <- NULL
x[1] <- "Blue"
x[2] <- "Green"
x[3] <- "Red"
x
## Section VII: Filtering ##
# Relational Operators
1 > 3
1 == 3
install.packages("slam")
install.packages("tm")
install.packages("wordcloud")
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
setwd("/Users/tianfansong/Desktop/GR5243/Spr2017-Proj1-songx706/lib")
folder.path="../data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
prez.out
speeches
prez.out
length(speeches)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path))
ff.all
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
tdm.all<-TermDocumentMatrix(ff.all)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
scale=c(5,0.5),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
scale=c(5,0.5),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
dtm <- DocumentTermMatrix(ff.all,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
dtm <- DocumentTermMatrix(ff.all,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
ff.dtm=tidy(dtm)
for(i in 1:length(speeches)){
#  #crude=stemDocument(ff.all[[i]])
#  crude=Corpus(VectorSource(ff.all[[i]]))
#  tdm <- TermDocumentMatrix(crude[1], list(wordLengths=c(3, Inf)))
#  m <- as.matrix(tdm)
#  v <- sort(rowSums(m),decreasing=TRUE)
#  d <- data.frame(word = names(v),freq=v)
png(paste("../output/", prez.out[i], ".png", sep=""),
width=300, height=300)
wordcloud(ff.dtm$term[ff.dtm$document==speeches[i]],
ff.dtm$count[ff.dtm$document==speeches[i]],
scale=c(5,0.5),
max.words=200,
min.freq=1,
random.order=FALSE,
rot.per=0,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Blues"),
main=prez.out[i])
dev.off()
}
View(tdm.tidy)
View(tdm.tidy)
View(tdm.tidy)
View(tdm.overall)
View(ff.dtm)
View(tdm.tidy)
install.packages("topicmodels")
install.packages("DT")
what<-o
cpi<-read.csv("cpiu-long.csv", header=T,skip = 2)
View(ff.dtm)
unlink('Desktop/GR5243/Spr2017-Proj1-songx706/lib/starter_cache', recursive = TRUE)
setwd("/Users/tianfansong/Desktop/GR5243/Spr2017-Proj1-songx706/lib")
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
folder.path="../data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path))
ff.all<-Corpus(DirSource(folder.path))
writeLines(as.character(ff.all[[30]]))
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removePunctuation)
tdm.all<-TermDocumentMatrix(ff.all)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
scale=c(5,0.5),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
dtm <- DocumentTermMatrix(ff.all,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
ff.dtm=tidy(dtm)
ff.all<-tm_map(ff.all, stemDocument)
for(i in 1:length(speeches)){
crude=stemDocument(ff.all[[i]])
crude=Corpus(VectorSource(ff.all[[i]]))
tdm <- TermDocumentMatrix(crude[1], list(wordLengths=c(3, Inf)))
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
png(paste("../output/", prez.out[i], ".png", sep=""),
width=300, height=300)
wordcloud(ff.dtm$term[ff.dtm$document==speeches[i]],
ff.dtm$count[ff.dtm$document==speeches[i]],
scale=c(5,0.5),
max.words=200,
min.freq=1,
random.order=FALSE,
rot.per=0,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Blues"),
main=prez.out[i])
dev.off()
}
n<-10
1+n
setwd("/Users/tianfansong/Desktop/GR5243/Spr2017-Proj1-songx706/lib")
library(tm)
library(wordcloud)
library(dplyr)
library(dplyr)
library(tidytext)
library(dplyr)
library(tidytext)
folder.path="../data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
ff.all<-Corpus(DirSource(folder.path))
ff.all<-Corpus(DirSource(folder.path))
writeLines(as.character(ff.all[[30]]))
writeLines(as.character(ff.all[[30]]))
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
tdm.all<-TermDocumentMatrix(ff.all)
tdm.tidy=tidy(tdm.all)
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
scale=c(5,0.5),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
dtm <- DocumentTermMatrix(ff.all,
control = list(weighting =
function(x)
weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
ff.dtm=tidy(dtm)
ff.dtm=tidy(dtm)
ff.all<-tm_map(ff.all, stripWhitespace)
set.seed(1) # Please don't remove this code!
rnorm(n = 5, mean = 10, sd = 3)
normal100 <- rnorm(n = 100)
mean(normal100)
sd(normal100)
hist(normal100)
normal10 <- rnorm(n = 10)
normal1000 <- rnorm(n = 1000)
normal10000 <- rnorm(n = 10000)
normal100000 <- rnorm(n = 100000)
library(topicmodels)
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
library(DT)
datatable(tdm.tidy, options = list(pageLength = 5))
k <- 5
ldaOut <-LDA(dtm,k, method=”Gibbs”, control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
ldaOut <-LDA(dtm,k, method=”Gibbs”, control=list(nstart=nstart, seed = seed, best=best, burnin= burnin, iter = iter, thin=thin))
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin= burnin, iter = iter, thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin= burnin, iter = iter, thin=thin))
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 5
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin= burnin, iter = iter, thin=thin))
dtm <- DocumentTermMatrix(tdm.tidy)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
dtm <- DocumentTermMatrix(tdm.tidy)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[ord]
write.csv(freq[ord],”word_freq.csv”)
write.csv(freq[ord],"word_freq.csv")
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 5
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin= burnin, iter = iter, thin=thin))
